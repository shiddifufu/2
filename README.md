# 算法基础
该仓库采用多项式朴素贝叶斯分类器，基于条件概率的特征独立性假设。以下是详细的解释：

# 多项式朴素贝叶斯模型
该模型通过引入多项式展开来估计概率分布。具体来说，模型假设每个类别的条件概率可以表示为：
<img src="https://github.com/shiddifufu/GitDemo/blob/master/%E6%96%B0%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9/3b5e4823ee2828953f36ea1e4bebba4d.jpeg" width="200">
其中，
Y
Y 表示输出（类别），
X
X 表示输入特征矩阵，
K
K 为多项式的次数（如泰勒展开或其他多项式形式）。
特征独立性假设
模型假设每个特征之间是相互独立的，这使得贝叶斯定理可以在线性组合下进行分类。
贝叶斯定理在邮件分类中的应用
邮件分类可视为文本分类问题，模型会将输入电子邮件转换为向量表示（如TF-IDF向量），然后通过多项式展开计算每个类别的条件概率，最终根据概率最高的类别进行预测。
2. 数据处理流程
数据处理是分类任务中的关键步骤，以下是常见预处理步骤及其实现逻辑：

分词处理（文本分割）
将输入邮件的文本分割成单词或子词。常用的方法包括：

有序词表（词典法）：基于预定义的词表进行分割，适用于高效分割但需要大量人工标注的场景。
最大重叠子串（CRT/CRF）：通过动态规划或CRF模型自动学习最优分割方式，适合处理复杂的语法和拼写错误。
停用词过滤
移除常见的无意义单词（如“是”、“在”等），以减少噪声对分类效果的影响。实现逻辑可以通过预定义停用词列表直接过滤。

文本降维与标准化

TF-IDF转换：将单词转换为加权向量，权重由其在文档中的重要性（TF）和出现频率（IDF）决定。
归一化处理：对特征向量进行标准化（如均值或方差标准化），以减少不同文本长度对模型的影响。
数据增强与平衡处理
如果数据集存在类别不平衡问题，可以通过以下方法改善：

数据层次增强（如随机截断、随机插入单词）。
重抽样技术（如过采样少数类或欠采样多数类）。
3. 特征构建过程
特征的选择和构建是影响分类性能的关键步骤，以下是两种主要方法及其对比：

高频词特征选择

原始方法：基于单词在文档中的出现频率进行筛选，假设高频词更能反映类别信息。
实现逻辑：统计每个单词的频率（如TF），并将频率高于阈值的单词作为特征输入模型。
TF-IDF特征加权

方法对比：TF-IDF不仅考虑词的出现次数，还结合了逆文档计数（IDF）来计算词的重要性，认为某些低频但高价值的词更能区分类别。
数学表达式：
T
F
−
I
D
F
(
w
)
=
(
T
F
(
w
)
)
×
(
I
D
F
(
w
)
)
TF−IDF(w)=(TF(w))×(IDF(w))
其中，
I
D
F
(
w
)
IDF(w) 可以通过先验计算文档中单词的出现频率（如全文档中的平均频率）。
特征切割与合并

对高频词和TF-IDF向量进行动态调整：例如，对长句子或长序列进行局部截断，确保每个特征的长度适中。
特征自动选择（如Lasso/L1正则化）

通过正则化方法（如L1范数）对特征进行自动筛选，删除对模型贡献最小的特征，以减少计算负担和噪声。
4. 特征切换机制
模型支持根据不同的任务需求动态切换特征类型：

基于频率的高频词：适用于类别间差异较大的场景（如情感分析）。
TF-IDF加权向量：更适合处理长文本或复杂语法的问题。
结合两种方法：可以通过校验指标（如准确率、F1值）选择最优特征方式。
5. 模型训练与预测
多项式展开的具体实现：可以使用手动编写的多项式层或利用现有的深度学习框架（如TensorFlow、PyTorch）。

损失函数设计：通常采用交叉熵损失，支持多类别分类。

训练步骤：

初始化模型参数（如多项式系数、嵌入矩阵）。
前向传播：将特征矩阵通过多项式层计算输出 logits。
后向传播：计算梯度并更新参数。
预测阶段：对输入文本进行处理后，通过模型输出每个类别的概率值，最终取概率最高的类别作为结果。